{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461b3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0     TV  Radio  Newspaper  Sales\n",
      "0             1  230.1   37.8       69.2   22.1\n",
      "1             2   44.5   39.3       45.1   10.4\n",
      "2             3   17.2   45.9       69.3    9.3\n",
      "3             4  151.5   41.3       58.5   18.5\n",
      "4             5  180.8   10.8       58.4   12.9\n",
      "..          ...    ...    ...        ...    ...\n",
      "195         196   38.2    3.7       13.8    7.6\n",
      "196         197   94.2    4.9        8.1    9.7\n",
      "197         198  177.0    9.3        6.4   12.8\n",
      "198         199  283.6   42.0       66.2   25.5\n",
      "199         200  232.1    8.6        8.7   13.4\n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing important libraries and modules and defining dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('Advertising.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d23225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "TV            0\n",
      "Radio         0\n",
      "Newspaper     0\n",
      "Sales         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for null_values\n",
    "null_values = df.isnull().sum()\n",
    "#count null values for eachn column\n",
    "print(null_values)\n",
    "#as shown there are 24 null values in the Income Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e42ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (200, 3)\n",
      "Shape of y: (200,)\n"
     ]
    }
   ],
   "source": [
    "#Define training Set\n",
    "X = df.iloc[:, 1:4].values  # All columns except the last one\n",
    "y = df.iloc[:, -1].values   # Only the last column\n",
    "\n",
    "# Displaying the shapes of X and y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5419074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (120, 3)\n",
      "the shape of the training set (target) is: (120,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (40, 3)\n",
      "the shape of the cross validation set (target) is: (40,)\n",
      "\n",
      "the shape of the test set (input) is: (40, 3)\n",
      "the shape of the test set (target) is: (40,)\n"
     ]
    }
   ],
   "source": [
    "#we see that the features of X are in different ranges so we scale the features of X so they can be in the same range\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)\n",
    "#splitting training set for model evaluation after fitting is done to check for bias(underfitting) or variance(overfitting)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d840e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 3) (120000,)\n"
     ]
    }
   ],
   "source": [
    "#increasing the training set size because the more training examples we have, the better our algorithm learns the training set\n",
    "#so we duplicate the input and target array by a 1000 times\n",
    "Xt = np.tile(x_train,(1000,1))\n",
    "Yt= np.tile(y_train,(1000))   \n",
    "print(Xt.shape, Yt.shape)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2ce4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47592034 -0.14612098 -0.9691538 ]\n",
      " [-0.14879261 -1.24000266 -0.97836017]\n",
      " [ 1.08311798 -1.29402151  0.29211789]]\n",
      "[15.6 11.2 12.3]\n",
      "{'alpha': 0.0001, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.01, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'invscaling', 'loss': 'squared_loss', 'max_iter': 1000, 'n_iter_no_change': 5, 'penalty': 'l2', 'power_t': 0.25, 'random_state': None, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "number of iterations completed: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "#creating the regression model and defining the maximum number of iterations\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "#fitting the model to the train set\n",
    "sgdr.fit(Xt, Yt)\n",
    "#the library automatically defines the loss function and the learning rate which can be seen when we print out sgdr\n",
    "print(sgdr.get_params())\n",
    "#we also see it's number of iterations before it reached the gloal minimum i.e(where the cost function J(w,b) is minimum)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59adc24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.80416486 2.70912258 0.05065966]\n",
      "[14.02843866]\n"
     ]
    }
   ],
   "source": [
    "#printing out the parameters\n",
    "#remember these are the parameters for the normalized X(input features)\n",
    "\n",
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(w_norm)\n",
    "print(b_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ebf05c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5179633031796171\n",
      "0.8047747041013155\n",
      "1.6549263901336626\n"
     ]
    }
   ],
   "source": [
    "#making predictions with model parameters for the train,cv and test set\n",
    "yhat_train = sgdr.predict(x_train)\n",
    "yhat_cv = sgdr.predict(x_cv)\n",
    "yhat_test = sgdr.predict(x_test)\n",
    "\n",
    "#calculating how well the model works on the train set and the cross validation set by finding the MSE(how much the prediction)\n",
    "#differs from the target for the train and validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "err_train = mean_squared_error(y_train, yhat_train) / 2\n",
    "err_cv = mean_squared_error(y_cv, yhat_cv) / 2\n",
    "err_test = mean_squared_error(y_test, yhat_test) / 2\n",
    "print(err_train)\n",
    "print(err_cv)\n",
    "print(err_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we see the model does relatively well for the train set and does even better for the cross-validation set. since there is\n",
    "#no high bias or high variance we can say this model is good for prediction\n",
    "#Note: for any new data that wants to use the model for prediction, this data has to be scaled "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
