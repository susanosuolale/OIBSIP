{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3cdcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        v1                                                 v2 Unnamed: 2  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "...    ...                                                ...        ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will ÃŒ_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "0           NaN        NaN  \n",
      "1           NaN        NaN  \n",
      "2           NaN        NaN  \n",
      "3           NaN        NaN  \n",
      "4           NaN        NaN  \n",
      "...         ...        ...  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n",
      "\n",
      "[5572 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing important libraries and modules and defining dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#adding the latin encoding for compatibility with the type of data set\n",
    "df =pd.read_csv('spam.csv',encoding='latin-1')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a3c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1               0\n",
      "v2               0\n",
      "Unnamed: 2    5522\n",
      "Unnamed: 3    5560\n",
      "Unnamed: 4    5566\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for null_values\n",
    "null_values = df.isnull().sum()\n",
    "#count null values for eachn column\n",
    "print(null_values)\n",
    "#as shown there are 24 null values in the Income Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4b8acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5572,)\n",
      "Shape of y: (5572,)\n",
      "Shape of X: (5572, 8672)\n",
      "Shape of y: (5572,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Define training Set\n",
    "X = df.iloc[:, 1].values  # the second column\n",
    "y = df.iloc[:, 0].values   # the first column\n",
    "\n",
    "\n",
    "# Displaying the shapes of X and y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "#encoding X and y because machine learning models only work with numbers\n",
    "#encoding the y with ham as 0 and spam as 1\n",
    "label_mapping = {\"ham\": 0, \"spam\": 1}\n",
    "y_enc = np.vectorize(label_mapping.get)(y)\n",
    "\n",
    "#we vectorize the input(X) email strings array by using count vectorizer in scikit-learn. this is done by,\n",
    "#breaking the text into individual words, then create a dictionary or array of all the unique words\n",
    "#count the occurence of each unique word in the dictionary or array in each email string training example\n",
    "vectorizer = CountVectorizer()\n",
    "x_bow = vectorizer.fit_transform(X)\n",
    "x_enc = x_bow.toarray()\n",
    "\n",
    "print(\"Shape of X:\", x_enc.shape)\n",
    "print(\"Shape of y:\", y_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d6d235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (3343, 8672)\n",
      "the shape of the training set (target) is: (3343,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (1114, 8672)\n",
      "the shape of the cross validation set (target) is: (1114,)\n",
      "\n",
      "the shape of the test set (input) is: (1115, 8672)\n",
      "the shape of the test set (target) is: (1115,)\n"
     ]
    }
   ],
   "source": [
    "#NOTE: we do not scale this data because they are all in the same ranges, if not we'd be using standard scaler from\n",
    "#scikit-learn to scale the data\n",
    "#splitting training set for model evaluation after fitting is done to check for bias(underfitting) or variance(overfitting)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x_enc, y_enc, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa96e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: we also do not need to increase the training set size because we already have a large training set size \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#creating the regression model\n",
    "lr_model = LogisticRegression()\n",
    "#fitting the logisitic regression model to the data\n",
    "lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff053c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.0032904576727490475\n",
      "cross validation error 0.02064631956912033\n",
      "test error 0.02780269058295959\n"
     ]
    }
   ],
   "source": [
    "#for model evaluation,we use this model paramters to predict the y labels for the training and cross validation set and checking for\n",
    "#the percentage of errors(how close the prediction is to the target). this will help us identify if the model has overfit or underfit i.e if the cross validation\n",
    "#has a high rate of error (much greater than the training set) then this means the model has high variance and has overfit the \n",
    "#set\n",
    "#however if the train set has very high rate of error then the model has high bias and has underfit\n",
    "\n",
    "#we then calculate the error for the train, cross-validation and test set with the fit model to check for \n",
    "#high bias(underfitting) and high variance(overfitting)\n",
    "err_train = 1 -(lr_model.score(x_train, y_train))\n",
    "err_cv = 1 -(lr_model.score(x_cv, y_cv))\n",
    "err_test = 1 -(lr_model.score(x_test, y_test))\n",
    "\n",
    "print(\"train error:\", err_train)\n",
    "print(\"cross validation error\", err_cv)\n",
    "print(\"test error\", err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we see the model does very well on the train, cross validation and test set therefore this model doesn't have high bias or high\n",
    "#variance and can be used for prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
