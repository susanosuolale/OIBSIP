{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7b4cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
      "0      1            5.1           3.5            1.4           0.2   \n",
      "1      2            4.9           3.0            1.4           0.2   \n",
      "2      3            4.7           3.2            1.3           0.2   \n",
      "3      4            4.6           3.1            1.5           0.2   \n",
      "4      5            5.0           3.6            1.4           0.2   \n",
      "..   ...            ...           ...            ...           ...   \n",
      "145  146            6.7           3.0            5.2           2.3   \n",
      "146  147            6.3           2.5            5.0           1.9   \n",
      "147  148            6.5           3.0            5.2           2.0   \n",
      "148  149            6.2           3.4            5.4           2.3   \n",
      "149  150            5.9           3.0            5.1           1.8   \n",
      "\n",
      "            Species  \n",
      "0       Iris-setosa  \n",
      "1       Iris-setosa  \n",
      "2       Iris-setosa  \n",
      "3       Iris-setosa  \n",
      "4       Iris-setosa  \n",
      "..              ...  \n",
      "145  Iris-virginica  \n",
      "146  Iris-virginica  \n",
      "147  Iris-virginica  \n",
      "148  Iris-virginica  \n",
      "149  Iris-virginica  \n",
      "\n",
      "[150 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing important libraries and modules and defining dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "df = pd.read_csv('Iris.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930e4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id               0\n",
      "SepalLengthCm    0\n",
      "SepalWidthCm     0\n",
      "PetalLengthCm    0\n",
      "PetalWidthCm     0\n",
      "Species          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#checking for null_values\n",
    "null_values = df.isnull().sum()\n",
    "#count null values for eachn column\n",
    "print(null_values)\n",
    "#as shown there are 24 null values in the Income Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9770fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "#Define training Set\n",
    "X = df.iloc[:, 1:5].values  # All columns except the last one\n",
    "y = df.iloc[:, -1].values   # Only the last column\n",
    "\n",
    "#encoding the values of y to be 0,1,2 to represent each class as a number because models only deal with numbers\n",
    "label_mapping = {\"Iris-setosa\": 0, \"Iris-virginica\": 1, \"Iris-versicolor\": 2}\n",
    "y_enc = np.vectorize(label_mapping.get)(y)\n",
    "\n",
    "# Displaying the shapes of X and y to verify and the values of y_enc\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y_enc.shape)\n",
    "print(y_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7830b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (90, 4)\n",
      "the shape of the training set (target) is: (90,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (30, 4)\n",
      "the shape of the cross validation set (target) is: (30,)\n",
      "\n",
      "the shape of the test set (input) is: (30, 4)\n",
      "the shape of the test set (target) is: (30,)\n"
     ]
    }
   ],
   "source": [
    "#NOTE: we do not scale this data because they are all in the same ranges, if not we'd be using standard scaler from\n",
    "#scikit-learn to scale the data\n",
    "#splitting training set for model evaluation after fitting is done to check for bias(underfitting) or variance(overfitting)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(X, y_enc, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177fdd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 4) (90000,)\n",
      "[0 1 1 ... 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "#increasing the training set size because the more training examples we have, the better our algorithm learns the training set\n",
    "#so we duplicate the input and target array by a 1000 times\n",
    "Xt = np.tile(x_train,(1000,1))\n",
    "Yt= np.tile(y_train,(1000))   \n",
    "print(Xt.shape, Yt.shape)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d474e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define tensorflow model with one hidden layer with three activations and activation function of relu(0 for the negative values\n",
    "#of z and linear for the positive values of z )\n",
    "#and an output layer with three activations representing the output's multiclass(i.e the three iris species) and an activation function of linear(values of\n",
    "#z remain the sdme)\n",
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(4,)),\n",
    "        Dense(3, activation='relu', name = 'layer1'),\n",
    "        Dense(3, activation='linear', name = 'layer2')\n",
    "     ]\n",
    ")\n",
    "model.summary()\n",
    "#shows you a breakdown of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f54d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 9s 96us/sample - loss: 0.2269\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 6s 70us/sample - loss: 0.0766\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 7s 79us/sample - loss: 0.0682\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 7s 79us/sample - loss: 0.0654\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 8s 84us/sample - loss: 0.0644\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 7s 82us/sample - loss: 0.0633\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 6s 71us/sample - loss: 0.0628\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 7s 79us/sample - loss: 0.0623\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 7s 72us/sample - loss: 0.0618\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 7s 83us/sample - loss: 0.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fbd0ddcda0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the loss function. for better accuracy during compilation, we set the output activation to be linear instead of softmax\n",
    "#and specified it in \"from_logits = true\"\n",
    "#we define the learning rate and use the adam optimizer which dynamically adjusts the learning rate when training\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")\n",
    "#fit the algorithm to this data\n",
    "model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221ca7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.07043517 -0.25067753 -0.8253138 ]\n",
      " [-0.4743376  -0.12899333 -1.3761055 ]\n",
      " [ 0.01337022 -0.26141864  2.732112  ]\n",
      " [ 0.15794885  0.4977454   4.6687155 ]] \n",
      "b1: [ 0.         0.        -4.7897816]\n",
      "W2:\n",
      " [[ -0.34927058  -0.7225988    0.2960987 ]\n",
      " [ -0.39196324  -0.32647347  -0.73963904]\n",
      " [-22.85709      2.7212315    0.03291858]] \n",
      "b2: [ 18.581818 -13.210989   6.718433]\n"
     ]
    }
   ],
   "source": [
    "#getting the parameters(weight and bias) of the fit data\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff905707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03333333333333333\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#for model evaluation, using this paramters to predict the y labels for the training and cross validation set and checking for\n",
    "#the percentage of errors. this will help us identify if the model has overfit or underfit i.e if the cross validation\n",
    "#has a high rate of error (much greater than the training set) then this means the model has high variance and has overfit the \n",
    "#set\n",
    "#however if the train set has very high rate of error then the model has high bias and has underfit\n",
    "\n",
    "predictions_train = model.predict(x_train)\n",
    "predictions_cv = model.predict(x_cv)\n",
    "predictions_test = model.predict(x_test)\n",
    "#make predictions(returns an array of numbers for each training example) on train, cross-validation and test set and categorize into classes\n",
    "#based on highest number in array\n",
    "def predict(predictions):\n",
    "    m,n = predictions.shape\n",
    "    yhat = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        #iterating through each prediction (which is a list of numbers representing each class for each training example)\n",
    "        #and predicting the class with the highest number as the class for that training example\n",
    "        yhat[i] = np.argmax(predictions[i])\n",
    "    return(yhat)\n",
    "yhat_train = predict(predictions_train)\n",
    "yhat_cv = predict(predictions_cv)\n",
    "yhat_test = predict(predictions_test)\n",
    "\n",
    "\n",
    "#checking for percentage of error for the train set and cv set\n",
    "def check_error(yhat, y):\n",
    "    count = 0\n",
    "    for i in range(len(y)):\n",
    "        if yhat[i] != y[i]:\n",
    "            count +=1\n",
    "    error = count/len(y)\n",
    "    return error\n",
    "\n",
    "err_train = check_error(yhat_train, y_train)\n",
    "err_cv = check_error(yhat_cv, y_cv)\n",
    "err_test = check_error(yhat_test, y_test)\n",
    "\n",
    "print(err_train)\n",
    "print(err_cv)\n",
    "print(err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70848213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see the model does very well on the training set but does even better on the cv set having an accuracy of a 100%\n",
    "#therefore we can confirm the model is a perfect fit "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
