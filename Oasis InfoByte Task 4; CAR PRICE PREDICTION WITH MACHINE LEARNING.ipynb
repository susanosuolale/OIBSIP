{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c1c5fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Car_Name  Year  Selling_Price  Present_Price  Driven_kms Fuel_Type  \\\n",
      "0       ritz  2014           3.35           5.59       27000    Petrol   \n",
      "1        sx4  2013           4.75           9.54       43000    Diesel   \n",
      "2       ciaz  2017           7.25           9.85        6900    Petrol   \n",
      "3    wagon r  2011           2.85           4.15        5200    Petrol   \n",
      "4      swift  2014           4.60           6.87       42450    Diesel   \n",
      "..       ...   ...            ...            ...         ...       ...   \n",
      "296     city  2016           9.50          11.60       33988    Diesel   \n",
      "297     brio  2015           4.00           5.90       60000    Petrol   \n",
      "298     city  2009           3.35          11.00       87934    Petrol   \n",
      "299     city  2017          11.50          12.50        9000    Diesel   \n",
      "300     brio  2016           5.30           5.90        5464    Petrol   \n",
      "\n",
      "    Selling_type Transmission  Owner  \n",
      "0         Dealer       Manual      0  \n",
      "1         Dealer       Manual      0  \n",
      "2         Dealer       Manual      0  \n",
      "3         Dealer       Manual      0  \n",
      "4         Dealer       Manual      0  \n",
      "..           ...          ...    ...  \n",
      "296       Dealer       Manual      0  \n",
      "297       Dealer       Manual      0  \n",
      "298       Dealer       Manual      0  \n",
      "299       Dealer       Manual      0  \n",
      "300       Dealer       Manual      0  \n",
      "\n",
      "[301 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing important libraries and modules and defining dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "df = pd.read_csv('car data.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36919d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Car_Name  Year  Selling_Price  Present_Price  Driven_kms  Fuel_Type  \\\n",
      "0          90  2014           3.35           5.59       27000          2   \n",
      "1          93  2013           4.75           9.54       43000          1   \n",
      "2          68  2017           7.25           9.85        6900          2   \n",
      "3          96  2011           2.85           4.15        5200          2   \n",
      "4          92  2014           4.60           6.87       42450          1   \n",
      "..        ...   ...            ...            ...         ...        ...   \n",
      "296        69  2016           9.50          11.60       33988          1   \n",
      "297        66  2015           4.00           5.90       60000          2   \n",
      "298        69  2009           3.35          11.00       87934          2   \n",
      "299        69  2017          11.50          12.50        9000          1   \n",
      "300        66  2016           5.30           5.90        5464          2   \n",
      "\n",
      "     Selling_type  Transmission  Owner  \n",
      "0               0             1      0  \n",
      "1               0             1      0  \n",
      "2               0             1      0  \n",
      "3               0             1      0  \n",
      "4               0             1      0  \n",
      "..            ...           ...    ...  \n",
      "296             0             1      0  \n",
      "297             0             1      0  \n",
      "298             0             1      0  \n",
      "299             0             1      0  \n",
      "300             0             1      0  \n",
      "\n",
      "[301 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#encoding all the non numeric columns in the df\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to non-numeric columns\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b8aac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (301, 8)\n",
      "Shape of y: (301,)\n"
     ]
    }
   ],
   "source": [
    "#Define training Set\n",
    "X =  df.loc[:, ~df.columns.isin(['Selling_Price'])].values  # All columns except the selling price\n",
    "y = df.iloc[:, 2].values   # Only the selling price\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4450b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (180, 8)\n",
      "the shape of the training set (target) is: (180,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (60, 8)\n",
      "the shape of the cross validation set (target) is: (60,)\n",
      "\n",
      "the shape of the test set (input) is: (61, 8)\n",
      "the shape of the test set (target) is: (61,)\n"
     ]
    }
   ],
   "source": [
    "#we see that the features of X are in different ranges so we scale the features of X so they can be in the same range\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(X)\n",
    "#splitting training set for model evaluation after fitting is done to check for bias(underfitting) or variance(overfitting)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e7e1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 8) (18000,)\n"
     ]
    }
   ],
   "source": [
    "#increasing the training set size because the more training examples we have, the better our algorithm learns the training set\n",
    "#so we duplicate the input and target array by a 100 times\n",
    "Xt = np.tile(x_train,(100,1))\n",
    "Yt= np.tile(y_train,(100))   \n",
    "print(Xt.shape, Yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bef3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define tensorflow model with one hidden layer with three activations and activation function of relu(0 for the negative values\n",
    "#of z and linear for the positive values of z )\n",
    "#and an output layer with one activation representing the selling price and an activation function of linear(values of\n",
    "#z remain the sdme)\n",
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(8,)),\n",
    "        Dense(3, activation='relu', name = 'layer1'),\n",
    "        Dense(1, activation='linear', name = 'layer2')\n",
    "     ]\n",
    ")\n",
    "model.summary()\n",
    "#shows you a breakdown of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f6720b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 2s 105us/sample - loss: 6.6396\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 1s 58us/sample - loss: 0.9880\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 1s 59us/sample - loss: 0.6590\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 1s 58us/sample - loss: 0.6140\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 1s 65us/sample - loss: 0.5983\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 1s 81us/sample - loss: 0.5966\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 2s 85us/sample - loss: 0.5992\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 1s 81us/sample - loss: 0.5987\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 1s 77us/sample - loss: 0.5966\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 1s 64us/sample - loss: 0.5928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2551ecb2630>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the loss function which is the mean squared error for linear regression\n",
    "#we define the learning rate and use the adam optimizer which dynamically adjusts the learning rate when training\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")\n",
    "#fit the model to this data\n",
    "model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "719f0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.3292535   0.96947753  1.2299649 ]\n",
      " [-0.499663    2.0780513   3.0503995 ]\n",
      " [ 1.0140072  -2.2137315   1.5880443 ]\n",
      " [ 0.28228396 -0.47331786 -0.7956308 ]\n",
      " [ 0.9388719  -0.5683351  -0.84688073]\n",
      " [-1.3616987  -0.7342973  -0.5245941 ]\n",
      " [ 1.2373908  -1.0367063  -1.3127818 ]\n",
      " [-0.7221538   0.27949357  0.49812976]] \n",
      "b1: [-0.11195582 -0.5359699   1.5092524 ]\n",
      "W2:\n",
      " [[ 0.7669378]\n",
      " [-1.8653992]\n",
      " [ 1.7208183]] \n",
      "b2: [0.642372]\n"
     ]
    }
   ],
   "source": [
    "W1_norm, b1_norm = model.get_layer(\"layer1\").get_weights() #the normalized weights and biases\n",
    "W2_norm, b2_norm = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1_norm, \"\\nb1:\", b1_norm)\n",
    "print(\"W2:\\n\", W2_norm, \"\\nb2:\", b2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e706e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 0.2961286344653438\n",
      "cv error 0.30679257735089477\n",
      "test error 0.7154994382967785\n"
     ]
    }
   ],
   "source": [
    "#making predictions with model parameters for the train,cv and test set to check for high bias(overfitting) and high variance\n",
    "#(underfitting)\n",
    "yhat_train = model.predict(x_train)\n",
    "yhat_cv = model.predict(x_cv)\n",
    "yhat_test = model.predict(x_test)\n",
    "\n",
    "#calculating how well the model works on the train set and the cross validation set by finding the MSE(how much the prediction\n",
    "#differs from the target) for the train and validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "err_train = mean_squared_error(y_train, yhat_train) / 2\n",
    "err_cv = mean_squared_error(y_cv, yhat_cv) / 2\n",
    "err_test = mean_squared_error(y_test, yhat_test) / 2\n",
    "print(\"train error\",err_train)\n",
    "print(\"cv error\",err_cv)\n",
    "print(\"test error\", err_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf56949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#even though the corss-valiadtion error is slightly higher than the train error we see however model does very weell\n",
    "#on all the different sets and therefore is a good fir for car prediction prices\n",
    "#The input features of a new car to be predicted mustr first be normalized "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
